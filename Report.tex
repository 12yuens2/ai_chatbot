\documentclass{article}
\usepackage[backend=biber]{biblatex}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{float}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{gensymb}
\addbibresource{references.bib}

\newcommand{\n}[0]{\\[\baselineskip]}

\title{CS3105 Imitation game Practical}
\date{2017-03-10}
\author{140011146}

\begin{document}

\maketitle

\pagenumbering{arabic}


\section{Introduction}
In this practical we were asked to create an AI chatbot that is able to have a text conversation with a human and from this reflect on the Turing test as a means to determine artificial intelligence.
\n
I have designed my chatbot using a variety of techniques and compared how well each technique conducts conversations to find the strengths and weaknesses of each approach.
\section{Design and techniques}
For my chatbot, I chose not to create a closed domain bot that produces good conversation only for specific topics or inputs and instead I explore many different techniques used to create chatbots to compare them. For each technique, I implement it myself without libraries. By implementing each algorithm from the ground up, I am able to better understand what happens under the hood and evaluate how well they perform.
\subsection{ELIZA}
The first technique I used for making the chatbot was the \texttt{ELIZA} string replacement method as detailed in \cite{eliza}. By pattern matching with regular expressions and giving hard coded responses based on the given template, the bot will respond by reflecting the user input, or telling the user to continue and say more. These responses make the most sense if the character of the bot is well defined. In the example of the \texttt{ELIZA} bot, its character of being a Rogerian therapist made its responses sensible. The responses are also guaranteed to be grammatically correct because they are hard coded by humans and not generated. 
\subsection{Markov chains} \label{markov}
In this approach, I used a Markov chain to generate a string of words. Tri-grams have the best outcome for generating text over uni-grams and bi-grams according to both \cite{cheating} and \cite{norvig}. Here are some results from my own Markov model on different n-gram lengths trained on \texttt{overheard.txt} from the \texttt{nltk} corpus:
\n
\textbf{Uni-gram}: 
\begin{itemize}
\item \textit{you're She's main one only even she You like are! have change a shorter. I Jovi's used gonna is be Madison why his Hey...}
\item \textit{their isn't dad. is mother as I confused. heck event. And the they but lunch}
\end{itemize}
\textbf{Bi-gram}:
\begin{itemize}
\item \textit{got now I me, family did says program at to Restaurant Korea.}
\item \textit{or minute with you my Lucy in them. near America. married! salad. yeah, Else. know?}
\end{itemize}
\textbf{Tri-gram}:
\begin{itemize}
\item \textit{You scale unless Lobster that but she was trying to tell me it's not even funny.}
\item \textit{diet man coffee, gots to go see a suspicious black package on my side}
\end{itemize}
It can be seen from this sample that tri-grams generate strings closer to that of English compared to bi-grams and uni-grams and that is why my implementation uses tri-grams (although it is general and can be changed to any \textit{n}-gram by changing a constant).
\begin{figure}[H]
\centering
\begin {tikzpicture}[-latex, auto, node distance = 4em, semithick, state/.style ={circle, draw, minimum size=5em, inner sep=0}]
\node[state] (C) [blue] {feel a};
\node[state] (A) [left=of C, blue] {I feel};
\node[state] (B) [right =of C, blue] {a breeze};
\node[state] (D) [below =of C] {feel the};
\node[state] (E) [right =of D] {the wind};
\path (A) edge [blue] node[below =0.15 cm, blue] {a} (C);
\path (C) edge [blue]  node[below =0.15 cm, blue] {breeze} (B);
\path (A) edge node[below =0.15 cm] {the} (D);
\path (D) edge node[below =0.15 cm] {wind} (E);

\end{tikzpicture}

\caption{Example of Markov chaining with the sentence \textit{I feel a breeze}.}\label{markov-eg1}
\end{figure}
To generate the strings, I create tri-grams of words from a corpus of text (overheard.txt). For each tri-gram, the first two words are a state in the Markov model and the last word used as a transition to move from one state to another. The next state to move to is the state that contains a bi-gram of the second and last word.
\n
In the example shown in Figure \ref{markov-eg1}, the graph is created by training the model on the two sentences \textit{I feel a breeze} and \textit{I feel the wind}. The sentences can then be generated by following the Markov chain to the end starting from any state. Given multiple choices for the next transition, select one using probability based on the frequency of that tri-gram occurring in the training text. In this example the two transitions have equal probability (1/2) so the transition is chosen by random.
\n
Obviously the strings created are not close to being good responses in a conversation, but this is a good starting point for being able to generate responses without the need to hard code each response. The training data used for this model is very important to the results. For example, training it on the Ubuntu Chat Corpus causes the model to generate strings about installing libraries and programs on Linux. Similarly, training the model on text like Shakespeare's plays will make it respond in Shakespearean English. To continue building on this model, I follow the approach taken by Hutchens \cite{cheating} in his paper, using keywords from user input as a seed to choose a state to start in and also reflecting keywords like \textit{I} and \textit{you}.
\subsubsection{Strengths}
The text is purely generated and are not hard coded responses. 
\subsubsection{Weaknesses}
Much of what comes out of the Markov chain does not make sense. With tri-grams it does at least have some grammar, but often it is still not very good. There is the issue that if the chain stops too early (say below a minimum length for the response), then it has to get the next state by random. This further makes the response more incomprehensible and without a very large dataset, it is difficult to guarantee this doesn't happen often. A larger corpus will improve the performance as it will be less likely to have to jump to another state by random and the probability distribution of each transition will be better defined. 
\subsection{Retrieval based with sentence similarity metrics} \label{retrieval}
The idea behind the retrieval based technique is a very simple one. Given a corpus of conversations, map each message with its response. Then when given user input, simply match the input with all mapped messages and return the response if a match was found. The obvious issue with is approach is how to match the user input to mapped messages. It is unlikely a user enters string similar to one we have stored, let alone enter a string that matches exactly. So to improve this method, I try different string distance metrics. As an \textbf{Edit-distance} function, I used the \textit{Levenshtein distance} and as a \textbf{Token-based} function, I used the \textit{Cosine similarity}.
\subsubsection{Cosine Similarity}
The Cosine similarity is defined as follows:
\begin{equation*}
similarity(s_{1}, s_{2}) = \frac{\mathbf{v_{1}} \cdot \mathbf{v_{2}}}{|\mathbf{v_{1}}|\times|\mathbf{v_{2}}|}
\end{equation*}
where $s_{1}, s_{2}$ are strings and $\mathbf{v_{1}}, \mathbf{v_{2}}$ are their vector representations \cite{irbook}.
\n
The dot product of vectors $\mathbf{x}, \mathbf{y}$ with length $N$ is defined as:
\begin{equation*}
\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^{N}\mathbf{x_{i}} \mathbf{y_{i}}
\end{equation*}
The magnitude of a vector $\mathbf{v}$ is defined as:
\begin{equation*}
|\mathbf{v}| = \sqrt{\Sigma_{i=1}^{N}\mathbf{v_{i}}^2}
\end{equation*}
The vector of a string is represented by the frequency of the words that appear in that string. The two vectors are created together, so words that appear in one string but not another are represented as a 0 in the vector. This metric is tied closely to find the cosine angle between the two vectors and hence a similarity value of 0 means the two strings do not share anything in common because the angle between the two vectors 90$^{\circ}$. A value of 1 means the two strings are identical as the angle between the vectors is 0. 

\subsubsection{Levenshtein Distance}
Instead of measuring the difference in the string using tokens (words), the Levenshtein distance measures the difference as the number of deletions, insertions or substitutions required to transform from $s_{1}$ to $s_{2}$ \cite{levenshtein}.
\n
The value the distance returns is the integer number of how many transformations are needed, so a larger Levenshtein distance means the strings are further apart. Although it may seem logical initially to use edit distances to find the similarity between strings, I found it was not easy to use the metric properly. My first thought was to divide by the length of the string to get a relative measure. However there are a few complications to this. Because it is two strings, which length do I use to divide by? Further, it takes very few edits to change from one word to another that completely changes the meaning of a sentence. For example for the words \textit{cat} and \textit{sat}, one is a noun and the other is a verb, but the distance between the two is only 1. There are further issues as pointed out in \cite{lev-bad} which make the static measure problematic such as prefixes and affixes of words counting for multiple edits instead of just one and the surface similarity of words not reflecting their true similarity .
\n
However, I found that this measure can still be useful for typos. Misspelling or mistyping words is a common issue in text chat and also an issue that will not be picked up by the token based distance approach. Hence I have kept this method with a small threshold (=5) in my bot to allow for typos in the user input. Increasing the threshold allow for more mistakes but also risk parsing similarities that don't exist. Below is a sample dialogue where the user has frequent typos, but the bot is still able to converse using hard coded responses by using the Levenshtein distance.
\n
\textbf{User}: Hrllo
\\ \textbf{Bot}: How are you doing?
\\ \textbf{User}: im good, wat about u?
\\ \textbf{Bot}: I'm good as well
\\ \textbf{User}: tahts good to jear
\\ \textbf{Bot}: Yes it is
\\ \textbf{User}: goodbyue
\\ \textbf{Bot}: Bye!

\subsubsection{Strengths}
The strength of this approach contrasts with the Markov approach from Section \ref{markov}. Where the Markov approach completely generated the response, only relying on chaining n-grams, this approach returns proper structured responses.
\n
Given a good conversation corpus, perhaps an entire log of a single person's messages for many years (to give the bot responses that would be more related than taking many random conversations from different people) the dialogue from this technique would give a more natural conversation compared to that of \texttt{ELIZA} as it does not have the same pre-defined responses in a static pattern because it is ``trained" on real conversations. To make it more realistic, responses could be deleted from memory once they have been used to ensure the bot never responded in the same way. All responses would eventually be wiped, but if the corpus is large enough, it is unlikely a user can exhaust the whole database of responses in a sitting.
\subsubsection{Weaknesses}
There are two jarring problems with this technique:
\begin{enumerate}
\item If there is not a match in the database even after the similarity functions then there is no response
\item The returned responses do not match the user input due to lack of context in the conversation and ``memory" or due to similarity matching poorly
\end{enumerate}
The first problem can be solved by having a fallback solution such as using Markov chains or \texttt{ELIZA} pattern matching. The second problem is a much bigger one. Take this sample conversation from training this technique on \texttt{overheard.txt}.
\n
\textbf{User}: How was your day?
\\ \textbf{Bot}: I went to the country today
\\ \textbf{User}: Where did you go?
\\ \textbf{Bot}: It slid down a crack between the counter and the stove
\n
Here the first response made sense, but in the second response, the Levenshtein distance allowed \textit{Where did you go?} to be close enough to matched with the response for \textit{Where did it go?}. The distance between these two sentences is only 3 (2 substitutions and 1 insertion) which is below the threshold of 5 that was set. Setting the threshold lower to resolve this issue is not a good idea because we may miss out on genuine matches and the same problem would still persist at a lower threshold. 
\n
Even if the input matches to a response directly, context from the conversation that was in the training corpus may be lost when simply output this way. Here is another example from the same training corpus.
\n
\textbf{User}: Hello
\\ \textbf{Bot}: What's up?
\\ \textbf{User}: I don't understand it
\\ \textbf{Bot}: My cat doesn't have a litter box. It uses the toilet. Does that help?
\n 
The response from the bot here doesn't make sense because it simply fetched a response. In the context of the original conversation, the response is valid, but here where there is no context, because we are just fetching the responses directly, it does not match with the flow of the conversation. 
\subsection{Naive Bayes classifier}
To improve on the retrieval based technique described earlier, I wanted to use a technique that could address the lack of context by making sure the bot will pick responses with the right context or on the same topic. Chapter 6 of the Python book on natural language processing \cite{nltk} gives many examples and techniques on classifying text. I wanted the technique to involve some form of ``training" like the previous techniques I've implemented using a corpus. \textbf{Naive Bayes classification} fit what I was looking for and was simple to implement.
\n

\section{Turing test}
From creating my chatbot and implementing and testing many different techniques, I didn't arrive at more of an understanding of the Turing test because I did not focus on trying to ``cheat" or pass the test. However, I have learned from my techniques how difficult it is to get the bot to truly learn or understand and how even a sophisticated chatbot that passes the Turing test can still be far from general intelligence.
\n

\section{Conclusion}
In conclusion, 
\printbibliography


\end{document}